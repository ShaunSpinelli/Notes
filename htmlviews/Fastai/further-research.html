<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Things to do further research on</title>
    <link type="text/css" rel="stylesheet" href="../assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="../assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="../assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="things-to-do-further-research-on"><a class="header-link" href="#things-to-do-further-research-on"></a>Things to do further research on</h1>
<p><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">Machine Learning Course</a></p>
<h2 id="concepts"><a class="header-link" href="#concepts"></a>Concepts</h2>
<ul class="list">
<li><p>precommpute</p>
</li>
<li><p>kaggl-cli on git hub</p>
</li>
</ul>
<h2 id="python-libaries"><a class="header-link" href="#python-libaries"></a>Python Libaries</h2>
<ul class="list">
<li><p>python pandas</p>
</li>
<li><p>learn matplot lib</p>
</li>
<li><p>seaborn</p>
</li>
</ul>
<h2 id="questions"><a class="header-link" href="#questions"></a>Questions</h2>
<p>If our validation loss and training set loss are very similar does that mean our model is generalised well ?</p>
<h2 id="readings"><a class="header-link" href="#readings"></a>Readings</h2>
<p><a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0">Optimal learning Rate</a></p>
<p><a href="https://medium.com/@apiltamang/case-study-a-world-class-image-classifier-for-dogs-and-cats-err-anything-9cf39ee4690e">Lesson 1 summary</a></p>
<p><a href="https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040">Guide to Pytorch</a></p>
<p><a href="https://miguel-data-sc.github.io/2017-11-05-first/">Visualing learning rate and batch size</a></p>
<p><a href="http://teleported.in/posts/decoding-resnet-architecture/">ResNet</a></p>
<p><a href="https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b">Learning rates</a></p>
<p><a href="https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e">Gradient Decent with restarts</a></p>
<p><a href="https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00">Transfer Learning/Differential LR</a></p>
<p><a href="https://towardsdatascience.com/structured-deep-learning-b8ca4138b848">Structured Deep Learning</a></p>
<p><a href="https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96">Image recogntion</a></p>
<p><a href="https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73">How do we train neural nets</a></p>
    </article>
  </body>
</html>
