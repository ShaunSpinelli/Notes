<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Lesson 5</title>
    <link type="text/css" rel="stylesheet" href="../assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="../assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="../assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="lesson-5"><a class="header-link" href="#lesson-5"></a>Lesson 5</h1>
<h2 id="topics"><a class="header-link" href="#topics"></a>Topics</h2>
<ul class="list">
<li><p>Matrix&#39;s Decomposition</p>
</li>
<li><p>weight decay</p>
</li>
</ul>
<p>Collaborative filter in movies example. Trying to find out if a user_1 would like a movie. Did other people who like similar movies to user_1 like this movie and  movies that were liked by people like user_1.</p>
<p>pytorch module</p>
<p>create a torch tensor <strong>what is a torch tensor</strong></p>
<pre class="hljs"><code>a = T([[<span class="hljs-number">1.</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
b = T([[<span class="hljs-number">2.</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">10</span>,<span class="hljs-number">10</span>]])</code></pre><p>where <code>a</code> and <code>b</code> are torch tensors</p>
<p> building a neural net layer , <strong>pytorch module</strong></p>
<pre class="hljs"><code><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DotProduct</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, u, m)</span>:</span> <span class="hljs-keyword">return</span> (u*m).sum(<span class="hljs-number">1</span>)

model=DotProduct()

model(a,b)</code></pre><p><code>DotProduct</code> is inheriting from <code>nn.Module</code></p>
<h2 id="movie-example-using-collaborative-filtering"><a class="header-link" href="#movie-example-using-collaborative-filtering"></a>Movie Example using Collaborative Filtering</h2>
<p>User ids may not be continuos ie: could have ids ranges from 1 to 100 but there may be some missing.So we not want to use the user id to look up the matching tensor column. So instead we  use the uniq user ids and match them to a new set of continuos user ids.<em>The same process and be used for movies</em></p>
<pre class="hljs"><code>
u_uniq = ratings.userId.unique()
user2idx = {o:i <span class="hljs-keyword">for</span> i,o <span class="hljs-keyword">in</span> enumerate(u_uniq)}
ratings.userId = ratings.userId.apply(<span class="hljs-keyword">lambda</span> x: user2idx[x]) <span class="hljs-comment">#replace user_id column with continuous index</span>

m_uniq = ratings.movieId.unique()
movie2idx = {o:i <span class="hljs-keyword">for</span> i,o <span class="hljs-keyword">in</span> enumerate(m_uniq)}
ratings.movieId = ratings.movieId.apply(<span class="hljs-keyword">lambda</span> x: movie2idx[x])

n_users=int(ratings.userId.nunique())
n_movies=int(ratings.movieId.nunique())
</code></pre><p>Our user and movie id&#39;s have no been mapped to continuos integers</p>
<p>Creating he embedding matrix</p>
<pre class="hljs"><code>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EmbeddingDot</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, n_users, n_movies)</span>:</span>
        super().__init__()
        self.u = nn.Embedding(n_users, n_factors)<span class="hljs-comment">#(rows,columns)</span>
        self.m = nn.Embedding(n_movies, n_factors)
        self.u.weight.data.uniform_(<span class="hljs-number">0</span>,<span class="hljs-number">0.05</span>) <span class="hljs-comment">#setting the values in the embedding matrix</span>
        self.m.weight.data.uniform_(<span class="hljs-number">0</span>,<span class="hljs-number">0.05</span>) <span class="hljs-comment">#randomly initalized embbede weight matrices</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, cats, conts)</span>:</span>
        users,movies = cats[:,<span class="hljs-number">0</span>],cats[:,<span class="hljs-number">1</span>]
        u,m = self.u(users),self.m(movies)
        <span class="hljs-keyword">return</span> (u*m).sum(<span class="hljs-number">1</span>)
</code></pre><p>using <a href="http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/">Kaiming He initializing</a> to find the starting values for the embedding matrix.</p>
<p>Actual embedding matrix is not a tensor but a variable but it also does automatic differention</p>
<p>Creating our data</p>
<pre class="hljs"><code>
x = ratings.drop([<span class="hljs-string">'rating'</span>, <span class="hljs-string">'timestamp'</span>],axis=<span class="hljs-number">1</span>)
y = ratings[<span class="hljs-string">'rating'</span>].astype(np.float32)
data = ColumnarModelData.from_data_frame(path, val_idxs, x, y, [<span class="hljs-string">'userId'</span>, <span class="hljs-string">'movieId'</span>], <span class="hljs-number">64</span>)
</code></pre><p>Setting our model and opt.</p>
<pre class="hljs"><code>
wd=<span class="hljs-number">1e-5</span>
model = EmbeddingDot(n_users, n_movies).cuda()
opt = optim.SGD(model.parameters(), <span class="hljs-number">1e-1</span>, weight_decay=wd, momentum=<span class="hljs-number">0.9</span>)
</code></pre><p>where</p>
<ul class="list">
<li><p>model is our embedding matrix</p>
</li>
<li><p>opt is updated our weights</p>
</li>
<li><p><code>wd</code> weight decay , change loss function</p>
</li>
</ul>
<p>fastai fit function</p>
<pre class="hljs"><code>
fit(model, data, <span class="hljs-number">3</span>, opt, F.mse_loss)
</code></pre><p>We can set our own learning rate annealing  <code>set_lrs(opt, 0.01)</code></p>
<p>Adding a bias 1:02:40</p>
<p><code>.squeeze</code> adds an additional unit axis. (pytorch function) Called broadcasting (we add a vector to a matrix) google <strong>numpy broadcasting</strong></p>
<p>continue at 140min</p>
    </article>
  </body>
</html>
