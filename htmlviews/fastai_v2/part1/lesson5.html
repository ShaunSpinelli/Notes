<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Lesson 5</title>
    <link type="text/css" rel="stylesheet" href="../../assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="../../assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="../../assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="lesson-5"><a class="header-link" href="#lesson-5"></a>Lesson 5</h1>
<p><a href="http://forums.fast.ai/t/wiki-lesson-5/9403">Lesson 5 Wiki</a></p>
<h2 id="topics"><a class="header-link" href="#topics"></a>Topics</h2>
<ul class="list">
<li><p>Collaborative filtering</p>
</li>
<li><p>Matrix&#39;s Decomposition</p>
</li>
<li><p>weight decay</p>
</li>
</ul>
<p>Collaborative filter in movies example. Trying to find out if a user_1 would like a movie. Did other people who like similar movies to user_1 like this movie and  movies that were liked by people like user_1.</p>
<p>pytorch module</p>
<p>create a torch tensor <strong>what is a torch tensor</strong> this the the fast.ai way</p>
<pre class="hljs"><code>a = T([[<span class="hljs-number">1.</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
b = T([[<span class="hljs-number">2.</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">10</span>,<span class="hljs-number">10</span>]])</code></pre><p>where <code>a</code> and <code>b</code> are torch tensors</p>
<p> building a neural net layer , <strong>pytorch module</strong></p>
<pre class="hljs"><code><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DotProduct</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, u, m)</span>:</span> <span class="hljs-keyword">return</span> (u*m).sum(<span class="hljs-number">1</span>)

model=DotProduct()

model(a,b)</code></pre><p><code>DotProduct</code> is inheriting from <code>nn.Module</code></p>
<h2 id="movie-example-using-collaborative-filtering"><a class="header-link" href="#movie-example-using-collaborative-filtering"></a>Movie Example using Collaborative Filtering</h2>
<p>User ids may not be continuos ie: could have ids ranges from 1 to 100 but there may be some missing.So we not want to use the user id to look up the matching tensor column. So instead we  use the uniq user ids and match them to a new set of continuos user ids.<em>The same process and be used for movies</em></p>
<pre class="hljs"><code>
u_uniq = ratings.userId.unique()
user2idx = {o:i <span class="hljs-keyword">for</span> i,o <span class="hljs-keyword">in</span> enumerate(u_uniq)}
ratings.userId = ratings.userId.apply(<span class="hljs-keyword">lambda</span> x: user2idx[x]) <span class="hljs-comment">#replace user_id column with continuous index</span>

m_uniq = ratings.movieId.unique()
movie2idx = {o:i <span class="hljs-keyword">for</span> i,o <span class="hljs-keyword">in</span> enumerate(m_uniq)}
ratings.movieId = ratings.movieId.apply(<span class="hljs-keyword">lambda</span> x: movie2idx[x])

n_users=int(ratings.userId.nunique())
n_movies=int(ratings.movieId.nunique())
</code></pre><p>Our user and movie id&#39;s have now been mapped to continuos integers</p>
<p>Creating he embedding matrix</p>
<pre class="hljs"><code>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EmbeddingDot</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, n_users, n_movies)</span>:</span>
        super().__init__()
        self.u = nn.Embedding(n_users, n_factors)<span class="hljs-comment">#(rows,columns)</span>
        self.m = nn.Embedding(n_movies, n_factors)
        self.u.weight.data.uniform_(<span class="hljs-number">0</span>,<span class="hljs-number">0.05</span>) <span class="hljs-comment">#setting the values in the embedding matrix</span>
        self.m.weight.data.uniform_(<span class="hljs-number">0</span>,<span class="hljs-number">0.05</span>) <span class="hljs-comment">#randomly initialized embedded weight matrices (_)sets it place</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, cats, conts)</span>:</span>
        users,movies = cats[:,<span class="hljs-number">0</span>],cats[:,<span class="hljs-number">1</span>]
        u,m = self.u(users),self.m(movies)
        <span class="hljs-keyword">return</span> (u*m).sum(<span class="hljs-number">1</span>)
</code></pre><p>using <a href="http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/">Kaiming He initializing</a> to find the starting values for the embedding matrix.</p>
<p>Actual embedding matrix is not a tensor but a variable but it also does automatic differentiation</p>
<p>Creating our data</p>
<pre class="hljs"><code>
x = ratings.drop([<span class="hljs-string">'rating'</span>, <span class="hljs-string">'timestamp'</span>],axis=<span class="hljs-number">1</span>)
y = ratings[<span class="hljs-string">'rating'</span>].astype(np.float32)
data = ColumnarModelData.from_data_frame(path, val_idxs, x, y, [<span class="hljs-string">'userId'</span>, <span class="hljs-string">'movieId'</span>], <span class="hljs-number">64</span>)
</code></pre><p>Setting our model and opt.</p>
<pre class="hljs"><code>
wd=<span class="hljs-number">1e-5</span>
model = EmbeddingDot(n_users, n_movies).cuda()
opt = optim.SGD(model.parameters(), <span class="hljs-number">1e-1</span>, weight_decay=wd, momentum=<span class="hljs-number">0.9</span>)
</code></pre><p>where</p>
<ul class="list">
<li><p>model is our embedding matrix</p>
</li>
<li><p>opt is updated our weights</p>
</li>
<li><p><code>wd</code> weight decay , change loss function</p>
</li>
</ul>
<p>fastai fit function</p>
<pre class="hljs"><code>
fit(model, data, <span class="hljs-number">3</span>, opt, F.mse_loss)
</code></pre><p>We can set our own learning rate annealing  <code>set_lrs(opt, 0.01)</code></p>
<p>Adding a bias 1:02:40</p>
<p><code>.squeeze</code> adds an additional unit axis. (pytorch function) Called broadcasting (we add a vector to a matrix) google <strong>numpy broadcasting</strong></p>
<p>bias -</p>
<h2 id="create-neural-net-collaborative-filter-filter"><a class="header-link" href="#create-neural-net-collaborative-filter-filter"></a>Create neural net collaborative filter filter</h2>
<p>concat embeddings for movie and user then feed into a neural net
 <em>check flow chart at 1:23</em></p>
<p> Are we updating the concated embeddings in our neural net?</p>
<h2 id="optimisers"><a class="header-link" href="#optimisers"></a>Optimisers</h2>
<p>1:36:30</p>
<p>grades excel doc (read from right to left)</p>
<p>chain rule is back propagation</p>
<p>1) basic - sgd</p>
<p>2) momentum  -  we hae another param we can tune</p>
<p>3) adam - uses momentum  of graident and momentum of gradient  sqaured (expontntialy weighted moving avergae) adaptive learning rate , weight decay</p>
    </article>
  </body>
</html>
