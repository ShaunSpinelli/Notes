# Things to do further research on

## Concepts

* precommpute

* kaggl-cli on git hub

##  Python Libaries

* python pandas

* learn matplot lib

* seaborn

## Questions

If our validation loss and training set loss are very similar does that mean our model is generalised well ?

## Readings

[Optimal learning Rate](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)

[Lesson 1 summary](https://medium.com/@apiltamang/case-study-a-world-class-image-classifier-for-dogs-and-cats-err-anything-9cf39ee4690e)

[Guide to Pytorch](https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040)

[Visualing learning rate and batch size](https://miguel-data-sc.github.io/2017-11-05-first/)

[ResNet](http://teleported.in/posts/decoding-resnet-architecture/)

[Learning rates](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)

[Gradient Decent with restarts](https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)

[Transfer Learning/Differential LR](https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00)

[Structured Deep Learning](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848)

[Image recogntion](https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96)

[How do we train neural nets](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73)