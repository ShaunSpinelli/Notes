# Things to do further research on

* kaggl-cli on git hub
* python pandas
* learn matplot lib
* precommpute

# Questions

If our validation loss and training set loss are very similar does that mean our model is generalised well ?

# Readings

[Optimal learning Rate](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)

[Lesson 1 summary](https://medium.com/@apiltamang/case-study-a-world-class-image-classifier-for-dogs-and-cats-err-anything-9cf39ee4690e)

[Guide to Pytorch](https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040)

[Visualing learning rate and batch size](https://miguel-data-sc.github.io/2017-11-05-first/)

[ResNet](http://teleported.in/posts/decoding-resnet-architecture/   )