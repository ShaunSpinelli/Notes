# FastAi Part 2

## Lesson one

### Matrix Multiplication


### Normalisation

Pull out some keys ideas from papers


### Basic architecture

### Basic training loop

feed data into model, Get predictions, Calculate loss from predictions using a loss function, Backward pass( to do what?? compute the gradient), subtract gradients * learning rate from parameters of the model. repeat. Why does this work?

#### Forward pass

#### Backward pass

gradients, chain rule

### Loss functions

### MSE -mean squared error

there is a passage in the 100 page ml book on mse i think

### Optimisers

what is the idea behind optimisers